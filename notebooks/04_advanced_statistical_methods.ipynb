{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Methods for Alaska OCS Lease Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates advanced statistical techniques applied to the Alaska OCS lease dataset:\n",
    "- **Bayesian Regression**: Uncertainty quantification for bid prediction\n",
    "- **Time Series Forecasting**: ARIMA and Prophet models for lease activity\n",
    "- **Survival Analysis**: Lease lifecycle and termination patterns\n",
    "- **Causal Inference**: Propensity score matching for policy impact\n",
    "\n",
    "These methods showcase sophisticated data science capabilities beyond traditional ML approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Statistical libraries\nfrom scipy import stats\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ntry:\n    from statsmodels.tsa.arima.model import ARIMA\n    from statsmodels.tsa.stattools import adfuller\n    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n    STATSMODELS_AVAILABLE = True\n    print(\"statsmodels available for time series analysis\")\nexcept ImportError:\n    STATSMODELS_AVAILABLE = False\n    print(\"statsmodels not available - using basic time series methods\")\n\n# Set plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"Statistical libraries loaded successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "df = pd.read_csv('../data/AK_Leases.csv')\n",
    "\n",
    "# Convert date columns\n",
    "date_columns = ['LEASE_EXPIR_DATE', 'LEASE_EFF_DATE', 'LEASE_EXPT_EXPIR', \n",
    "               'LEASE_STATUS_CHANGE_DT', 'LSE_STAT_EFF_DT', 'SALE_DATE']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Create derived features\n",
    "df['SALE_YEAR'] = df['SALE_DATE'].dt.year\n",
    "df['BID_PER_HECTARE'] = df['BID_AMOUNT'] / df['CURRENT_AREA'].replace(0, np.nan)\n",
    "df['LEASE_DURATION'] = (df['LEASE_EXPIR_DATE'] - df['LEASE_EFF_DATE']).dt.days\n",
    "df['IS_ACTIVE'] = (df['LEASE_IS_ACTIVE'] == 'Y').astype(int)\n",
    "df['DECADE'] = (df['SALE_YEAR'] // 10) * 10\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} leases, {df.shape[1]} columns\")\n",
    "print(f\"Date range: {df['SALE_DATE'].min()} to {df['SALE_DATE'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Regression for Bid Prediction\n",
    "\n",
    "Bayesian regression provides uncertainty quantification for predictions, which is crucial for financial decision-making in lease bidding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Bayesian regression\n",
    "# Filter valid data for modeling\n",
    "model_data = df[\n",
    "    (df['BID_AMOUNT'] > 0) & \n",
    "    (df['CURRENT_AREA'] > 0) & \n",
    "    (df['ROYALTY_RATE'].notna()) & \n",
    "    (df['PRIMARY_TERM'].notna()) &\n",
    "    (df['SALE_YEAR'] >= 1980)  # Focus on more recent data\n",
    "].copy()\n",
    "\n",
    "# Log transform bid amount for better modeling\n",
    "model_data['LOG_BID'] = np.log(model_data['BID_AMOUNT'])\n",
    "\n",
    "# Standardize predictors\n",
    "scaler = StandardScaler()\n",
    "predictors = ['CURRENT_AREA', 'ROYALTY_RATE', 'PRIMARY_TERM', 'SALE_YEAR']\n",
    "X = scaler.fit_transform(model_data[predictors])\n",
    "y = model_data['LOG_BID'].values\n",
    "\n",
    "print(f\"Bayesian regression dataset: {len(model_data)} observations\")\n",
    "print(f\"Features: {predictors}\")\n",
    "print(f\"Target: LOG_BID (log-transformed bid amount)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Bayesian Linear Regression using analytical approach\n",
    "# Since PyMC might not be available, we'll use a conjugate prior approach\n",
    "\n",
    "class BayesianLinearRegression:\n",
    "    \"\"\"Bayesian Linear Regression with conjugate priors\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_0=1e-6, beta_0=1e-6):\n",
    "        self.alpha_0 = alpha_0  # Prior precision parameter\n",
    "        self.beta_0 = beta_0    # Prior noise precision parameter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Bayesian linear regression\"\"\"\n",
    "        n, p = X.shape\n",
    "        \n",
    "        # Add intercept term\n",
    "        X_design = np.column_stack([np.ones(n), X])\n",
    "        \n",
    "        # Prior parameters\n",
    "        m_0 = np.zeros(p + 1)  # Prior mean\n",
    "        S_0 = self.alpha_0 * np.eye(p + 1)  # Prior precision matrix\n",
    "        \n",
    "        # Posterior parameters\n",
    "        S_n = S_0 + self.beta_0 * X_design.T @ X_design\n",
    "        self.cov_n = np.linalg.inv(S_n)  # Posterior covariance\n",
    "        self.m_n = self.cov_n @ (S_0 @ m_0 + self.beta_0 * X_design.T @ y)  # Posterior mean\n",
    "        \n",
    "        # Noise precision posterior\n",
    "        self.a_n = self.alpha_0 + n / 2\n",
    "        residual = y - X_design @ self.m_n\n",
    "        self.b_n = self.beta_0 + 0.5 * (residual.T @ residual + \n",
    "                                       (self.m_n - m_0).T @ S_0 @ (self.m_n - m_0))\n",
    "        \n",
    "        self.X_design = X_design\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_new, n_samples=1000):\n",
    "        \"\"\"Generate posterior predictive samples\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "            \n",
    "        n_new = X_new.shape[0]\n",
    "        X_new_design = np.column_stack([np.ones(n_new), X_new])\n",
    "        \n",
    "        # Sample coefficients from posterior\n",
    "        coef_samples = np.random.multivariate_normal(self.m_n, self.cov_n, n_samples)\n",
    "        \n",
    "        # Generate predictions\n",
    "        pred_samples = X_new_design @ coef_samples.T\n",
    "        \n",
    "        # Add noise\n",
    "        noise_precision = np.random.gamma(self.a_n, 1/self.b_n, n_samples)\n",
    "        noise_std = 1 / np.sqrt(noise_precision)\n",
    "        \n",
    "        pred_with_noise = pred_samples + np.random.normal(0, noise_std, pred_samples.shape)\n",
    "        \n",
    "        return pred_with_noise\n",
    "    \n",
    "    def get_coefficients_summary(self, feature_names):\n",
    "        \"\"\"Get coefficient posterior summaries\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "            \n",
    "        coef_std = np.sqrt(np.diag(self.cov_n))\n",
    "        \n",
    "        summary = pd.DataFrame({\n",
    "            'Feature': ['Intercept'] + feature_names,\n",
    "            'Mean': self.m_n,\n",
    "            'Std': coef_std,\n",
    "            'CI_2.5%': self.m_n - 1.96 * coef_std,\n",
    "            'CI_97.5%': self.m_n + 1.96 * coef_std\n",
    "        })\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Fit Bayesian regression\n",
    "bayes_reg = BayesianLinearRegression(alpha_0=0.01, beta_0=1.0)\n",
    "bayes_reg.fit(X, y)\n",
    "\n",
    "# Get coefficient summaries\n",
    "coef_summary = bayes_reg.get_coefficients_summary(predictors)\n",
    "print(\"Bayesian Regression Coefficient Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(coef_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with uncertainty\n",
    "# Use a subset for prediction demonstration\n",
    "test_indices = np.random.choice(len(X), size=100, replace=False)\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# Generate posterior predictive samples\n",
    "pred_samples = bayes_reg.predict(X_test, n_samples=2000)\n",
    "\n",
    "# Calculate prediction intervals\n",
    "pred_mean = np.mean(pred_samples, axis=1)\n",
    "pred_lower = np.percentile(pred_samples, 2.5, axis=1)\n",
    "pred_upper = np.percentile(pred_samples, 97.5, axis=1)\n",
    "pred_std = np.std(pred_samples, axis=1)\n",
    "\n",
    "# Create prediction results dataframe\n",
    "pred_results = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': pred_mean,\n",
    "    'Lower_95%': pred_lower,\n",
    "    'Upper_95%': pred_upper,\n",
    "    'Prediction_Std': pred_std,\n",
    "    'Within_CI': (y_test >= pred_lower) & (y_test <= pred_upper)\n",
    "})\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(np.mean((y_test - pred_mean)**2))\n",
    "mae = np.mean(np.abs(y_test - pred_mean))\n",
    "coverage = pred_results['Within_CI'].mean()\n",
    "\n",
    "print(f\"Bayesian Regression Performance:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"95% CI Coverage: {coverage:.2%}\")\n",
    "print(f\"Average Prediction Uncertainty (std): {pred_std.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bayesian predictions with uncertainty\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Bayesian Regression Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Predicted vs Actual with uncertainty\n",
    "axes[0,0].scatter(pred_results['Actual'], pred_results['Predicted'], alpha=0.6, color='blue')\n",
    "axes[0,0].errorbar(pred_results['Actual'], pred_results['Predicted'], \n",
    "                   yerr=1.96*pred_results['Prediction_Std'], \n",
    "                   fmt='none', alpha=0.3, color='red')\n",
    "axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.8)\n",
    "axes[0,0].set_xlabel('Actual Log(Bid Amount)')\n",
    "axes[0,0].set_ylabel('Predicted Log(Bid Amount)')\n",
    "axes[0,0].set_title('Predictions with 95% Uncertainty Intervals')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Coefficient posterior distributions\n",
    "coef_plot_data = coef_summary.iloc[1:]  # Exclude intercept for clarity\n",
    "y_pos = range(len(coef_plot_data))\n",
    "axes[0,1].errorbar(coef_plot_data['Mean'], y_pos, \n",
    "                   xerr=[coef_plot_data['Mean'] - coef_plot_data['CI_2.5%'],\n",
    "                        coef_plot_data['CI_97.5%'] - coef_plot_data['Mean']], \n",
    "                   fmt='o', capsize=5, color='darkgreen')\n",
    "axes[0,1].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0,1].set_yticks(y_pos)\n",
    "axes[0,1].set_yticklabels(coef_plot_data['Feature'])\n",
    "axes[0,1].set_xlabel('Coefficient Value')\n",
    "axes[0,1].set_title('Coefficient Posterior Distributions (95% CI)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Prediction uncertainty distribution\n",
    "axes[1,0].hist(pred_results['Prediction_Std'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1,0].axvline(pred_results['Prediction_Std'].mean(), color='red', linestyle='--', \n",
    "                 label=f'Mean: {pred_results[\"Prediction_Std\"].mean():.3f}')\n",
    "axes[1,0].set_xlabel('Prediction Standard Deviation')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title('Distribution of Prediction Uncertainty')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals with uncertainty bands\n",
    "residuals = pred_results['Actual'] - pred_results['Predicted']\n",
    "axes[1,1].scatter(pred_results['Predicted'], residuals, alpha=0.6, color='purple')\n",
    "axes[1,1].errorbar(pred_results['Predicted'], residuals, \n",
    "                   yerr=1.96*pred_results['Prediction_Std'], \n",
    "                   fmt='none', alpha=0.3, color='gray')\n",
    "axes[1,1].axhline(0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[1,1].set_xlabel('Predicted Log(Bid Amount)')\n",
    "axes[1,1].set_ylabel('Residuals')\n",
    "axes[1,1].set_title('Residuals with Uncertainty Bands')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nKey Insights from Bayesian Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "significant_features = coef_summary[\n",
    "    (coef_summary['CI_2.5%'] > 0) | (coef_summary['CI_97.5%'] < 0)\n",
    "]['Feature'].tolist()\n",
    "print(f\"Statistically significant features: {significant_features}\")\n",
    "print(f\"Model provides uncertainty estimates for all predictions\")\n",
    "print(f\"95% confidence intervals have {coverage:.1%} actual coverage\")\n",
    "print(f\"Average prediction uncertainty: ±{pred_std.mean():.3f} log units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Forecasting for Lease Activity\n",
    "\n",
    "Time series analysis helps predict future leasing patterns and identify cyclical behaviors in the oil and gas market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series data\n",
    "# Aggregate lease activity by year and quarter\n",
    "ts_data = df.groupby(df['SALE_DATE'].dt.year).agg({\n",
    "    'LEASE_NUMBER': 'count',\n",
    "    'BID_AMOUNT': 'sum',\n",
    "    'CURRENT_AREA': 'sum',\n",
    "    'IS_ACTIVE': 'mean'\n",
    "}).rename(columns={\n",
    "    'LEASE_NUMBER': 'num_leases',\n",
    "    'BID_AMOUNT': 'total_bid_value',\n",
    "    'CURRENT_AREA': 'total_area',\n",
    "    'IS_ACTIVE': 'active_rate'\n",
    "})\n",
    "\n",
    "# Remove years with very few observations or outliers\n",
    "ts_data = ts_data[(ts_data.index >= 1980) & (ts_data.index <= 2020)]\n",
    "ts_data.index = pd.to_datetime(ts_data.index, format='%Y')\n",
    "\n",
    "print(f\"Time series data prepared: {len(ts_data)} years\")\n",
    "print(f\"Variables: {list(ts_data.columns)}\")\n",
    "print(f\"Date range: {ts_data.index.min().year} to {ts_data.index.max().year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series decomposition\n",
    "# Focus on number of leases as primary metric\n",
    "lease_ts = ts_data['num_leases'].resample('Y').sum()\n",
    "\n",
    "# Perform seasonal decomposition (treating as annual data with potential cyclical patterns)\n",
    "# For demonstration, we'll use a simple trend analysis\n",
    "\n",
    "def decompose_trend(series, window=5):\n",
    "    \"\"\"Simple trend decomposition using moving averages\"\"\"\n",
    "    trend = series.rolling(window=window, center=True).mean()\n",
    "    detrended = series - trend\n",
    "    residual = detrended - detrended.rolling(window=3, center=True).mean()\n",
    "    \n",
    "    return trend, detrended, residual\n",
    "\n",
    "trend, detrended, residual = decompose_trend(lease_ts)\n",
    "\n",
    "# Visualize time series components\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "fig.suptitle('Time Series Decomposition: Annual Lease Activity', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original series\n",
    "axes[0].plot(lease_ts.index, lease_ts.values, 'b-', linewidth=2)\n",
    "axes[0].set_title('Original Time Series')\n",
    "axes[0].set_ylabel('Number of Leases')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend\n",
    "axes[1].plot(trend.index, trend.values, 'r-', linewidth=2)\n",
    "axes[1].set_title('Trend Component')\n",
    "axes[1].set_ylabel('Trend')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Detrended (cyclical + irregular)\n",
    "axes[2].plot(detrended.index, detrended.values, 'g-', linewidth=1.5)\n",
    "axes[2].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('Detrended Component')\n",
    "axes[2].set_ylabel('Detrended Values')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "axes[3].plot(residual.index, residual.values, 'purple', linewidth=1)\n",
    "axes[3].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[3].set_title('Residual Component')\n",
    "axes[3].set_ylabel('Residuals')\n",
    "axes[3].set_xlabel('Year')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA modeling\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Test for stationarity\n",
    "def check_stationarity(series, title):\n",
    "    \"\"\"Check if time series is stationary using Augmented Dickey-Fuller test\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f\"\\nStationarity Test for {title}:\")\n",
    "    print(f\"ADF Statistic: {result[0]:.6f}\")\n",
    "    print(f\"p-value: {result[1]:.6f}\")\n",
    "    print(f\"Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"\\t{key}: {value:.6f}\")\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Series is stationary\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Series is non-stationary\")\n",
    "        return False\n",
    "\n",
    "# Check stationarity of original series\n",
    "is_stationary = check_stationarity(lease_ts, \"Number of Leases\")\n",
    "\n",
    "# If non-stationary, difference the series\n",
    "if not is_stationary:\n",
    "    lease_ts_diff = lease_ts.diff().dropna()\n",
    "    is_stationary_diff = check_stationarity(lease_ts_diff, \"First Differenced Series\")\n",
    "    modeling_series = lease_ts_diff\n",
    "else:\n",
    "    modeling_series = lease_ts\n",
    "\n",
    "# Plot ACF and PACF for model identification\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ARIMA Model Identification', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original series\n",
    "axes[0,0].plot(lease_ts)\n",
    "axes[0,0].set_title('Original Series')\n",
    "axes[0,0].set_ylabel('Number of Leases')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Differenced series (if applicable)\n",
    "if not is_stationary:\n",
    "    axes[0,1].plot(modeling_series)\n",
    "    axes[0,1].set_title('First Differenced Series')\n",
    "else:\n",
    "    axes[0,1].plot(modeling_series)\n",
    "    axes[0,1].set_title('Modeling Series')\n",
    "axes[0,1].set_ylabel('Values')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# ACF\n",
    "plot_acf(modeling_series.dropna(), ax=axes[1,0], lags=min(15, len(modeling_series)//4))\n",
    "axes[1,0].set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PACF\n",
    "plot_pacf(modeling_series.dropna(), ax=axes[1,1], lags=min(15, len(modeling_series)//4))\n",
    "axes[1,1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARIMA model\n",
    "# Try different ARIMA specifications\n",
    "from itertools import product\n",
    "\n",
    "def evaluate_arima_models(series, p_values, d_values, q_values):\n",
    "    \"\"\"Evaluate different ARIMA model specifications\"\"\"\n",
    "    best_aic = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    results = []\n",
    "    \n",
    "    for p, d, q in product(p_values, d_values, q_values):\n",
    "        try:\n",
    "            model = ARIMA(series, order=(p, d, q))\n",
    "            fitted_model = model.fit()\n",
    "            aic = fitted_model.aic\n",
    "            results.append({'p': p, 'd': d, 'q': q, 'AIC': aic})\n",
    "            \n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_params = (p, d, q)\n",
    "                best_model = fitted_model\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return best_model, best_params, pd.DataFrame(results)\n",
    "\n",
    "# Grid search for best ARIMA parameters\n",
    "p_values = range(0, 4)\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 4)\n",
    "\n",
    "print(\"Evaluating ARIMA models...\")\n",
    "best_arima, best_params, model_results = evaluate_arima_models(\n",
    "    lease_ts, p_values, d_values, q_values\n",
    ")\n",
    "\n",
    "print(f\"\\nBest ARIMA model: ARIMA{best_params}\")\n",
    "print(f\"AIC: {best_arima.aic:.2f}\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(best_arima.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA forecasting and validation\n",
    "# Split data for validation\n",
    "train_size = int(0.8 * len(lease_ts))\n",
    "train_ts = lease_ts.iloc[:train_size]\n",
    "test_ts = lease_ts.iloc[train_size:]\n",
    "\n",
    "# Fit model on training data\n",
    "train_model = ARIMA(train_ts, order=best_params)\n",
    "fitted_train_model = train_model.fit()\n",
    "\n",
    "# Generate forecasts\n",
    "n_forecast = len(test_ts)\n",
    "forecast_result = fitted_train_model.get_forecast(steps=n_forecast)\n",
    "forecast_values = forecast_result.predicted_mean\n",
    "forecast_ci = forecast_result.conf_int()\n",
    "\n",
    "# Calculate forecast accuracy\n",
    "mae = np.mean(np.abs(test_ts.values - forecast_values.values))\n",
    "rmse = np.sqrt(np.mean((test_ts.values - forecast_values.values)**2))\n",
    "mape = np.mean(np.abs((test_ts.values - forecast_values.values) / test_ts.values)) * 100\n",
    "\n",
    "print(f\"ARIMA Forecast Accuracy:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Visualize forecasts\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot historical data\n",
    "plt.plot(train_ts.index, train_ts.values, 'b-', label='Training Data', linewidth=2)\n",
    "plt.plot(test_ts.index, test_ts.values, 'g-', label='Actual Test Data', linewidth=2)\n",
    "\n",
    "# Plot forecasts\n",
    "plt.plot(test_ts.index, forecast_values.values, 'r-', label='ARIMA Forecast', linewidth=2)\n",
    "plt.fill_between(test_ts.index, \n",
    "                forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], \n",
    "                color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "plt.axvline(x=train_ts.index[-1], color='black', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "plt.title('ARIMA Time Series Forecasting: Lease Activity', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Leases')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate future forecasts\n",
    "future_forecast = fitted_train_model.get_forecast(steps=5)\n",
    "future_values = future_forecast.predicted_mean\n",
    "future_ci = future_forecast.conf_int()\n",
    "\n",
    "print(\"\\n5-Year Forecast:\")\n",
    "print(\"=\" * 30)\n",
    "for i, (date, value, lower, upper) in enumerate(zip(\n",
    "    pd.date_range(lease_ts.index[-1] + pd.DateOffset(years=1), periods=5, freq='Y'),\n",
    "    future_values, future_ci.iloc[:, 0], future_ci.iloc[:, 1]\n",
    ")):\n",
    "    print(f\"{date.year}: {value:.0f} leases (95% CI: {lower:.0f}-{upper:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Survival Analysis for Lease Lifecycle\n",
    "\n",
    "Survival analysis models the duration until lease termination, providing insights into lease longevity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare survival analysis data\n",
    "# Calculate lease duration and censoring indicators\n",
    "\n",
    "survival_data = df[\n",
    "    (df['LEASE_EFF_DATE'].notna()) & \n",
    "    (df['ROYALTY_RATE'].notna()) &\n",
    "    (df['CURRENT_AREA'] > 0) &\n",
    "    (df['SALE_YEAR'] >= 1990)\n",
    "].copy()\n",
    "\n",
    "# Calculate duration\n",
    "# For active leases, use current date as censoring point\n",
    "current_date = pd.Timestamp('2023-01-01')\n",
    "\n",
    "def calculate_duration_and_event(row):\n",
    "    \"\"\"Calculate lease duration and event indicator\"\"\"\n",
    "    start_date = row['LEASE_EFF_DATE']\n",
    "    \n",
    "    if row['IS_ACTIVE'] == 1:  # Active lease (censored)\n",
    "        duration = (current_date - start_date).days\n",
    "        event = 0  # Censored\n",
    "    else:  # Terminated lease\n",
    "        end_date = row.get('LEASE_STATUS_CHANGE_DT', current_date)\n",
    "        if pd.isna(end_date):\n",
    "            end_date = current_date\n",
    "        duration = (end_date - start_date).days\n",
    "        event = 1  # Event occurred\n",
    "    \n",
    "    return max(duration, 1)  # Ensure positive duration\n",
    "\n",
    "# Apply duration calculation\n",
    "survival_data['duration_days'] = survival_data.apply(\n",
    "    lambda row: calculate_duration_and_event(row), axis=1\n",
    ")\n",
    "\n",
    "# Event indicator (1 = lease terminated, 0 = still active/censored)\n",
    "survival_data['event'] = (survival_data['IS_ACTIVE'] == 0).astype(int)\n",
    "\n",
    "# Convert to years for interpretability\n",
    "survival_data['duration_years'] = survival_data['duration_days'] / 365.25\n",
    "\n",
    "# Add covariates\n",
    "survival_data['log_area'] = np.log(survival_data['CURRENT_AREA'])\n",
    "survival_data['high_royalty'] = (survival_data['ROYALTY_RATE'] > survival_data['ROYALTY_RATE'].median()).astype(int)\n",
    "survival_data['decade_90s'] = (survival_data['DECADE'] == 1990).astype(int)\n",
    "survival_data['decade_2000s'] = (survival_data['DECADE'] == 2000).astype(int)\n",
    "\n",
    "print(f\"Survival analysis dataset: {len(survival_data)} leases\")\n",
    "print(f\"Events (terminations): {survival_data['event'].sum()}\")\n",
    "print(f\"Censored (active): {(1-survival_data['event']).sum()}\")\n",
    "print(f\"Censoring rate: {(1-survival_data['event']).mean():.1%}\")\n",
    "print(f\"Mean duration: {survival_data['duration_years'].mean():.1f} years\")\n",
    "print(f\"Median duration: {survival_data['duration_years'].median():.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaplan-Meier survival estimation\n",
    "# Since we might not have lifelines, implement basic survival analysis\n",
    "\n",
    "class KaplanMeierEstimator:\n",
    "    \"\"\"Simple Kaplan-Meier survival function estimator\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.survival_function = None\n",
    "        self.timeline = None\n",
    "    \n",
    "    def fit(self, durations, events):\n",
    "        \"\"\"Fit Kaplan-Meier estimator\"\"\"\n",
    "        # Sort by duration\n",
    "        sorted_idx = np.argsort(durations)\n",
    "        durations_sorted = durations[sorted_idx]\n",
    "        events_sorted = events[sorted_idx]\n",
    "        \n",
    "        # Find unique event times\n",
    "        unique_times = np.unique(durations_sorted[events_sorted == 1])\n",
    "        \n",
    "        # Calculate survival probabilities\n",
    "        survival_probs = []\n",
    "        n_at_risk = len(durations)\n",
    "        cum_survival = 1.0\n",
    "        \n",
    "        timeline = [0] + unique_times.tolist()\n",
    "        survival_function = [1.0]\n",
    "        \n",
    "        for t in unique_times:\n",
    "            # Number of events at time t\n",
    "            n_events = np.sum((durations_sorted == t) & (events_sorted == 1))\n",
    "            \n",
    "            # Update survival probability\n",
    "            if n_at_risk > 0:\n",
    "                cum_survival *= (1 - n_events / n_at_risk)\n",
    "            \n",
    "            survival_function.append(cum_survival)\n",
    "            \n",
    "            # Update risk set\n",
    "            n_at_risk -= np.sum(durations_sorted == t)\n",
    "        \n",
    "        self.timeline = np.array(timeline)\n",
    "        self.survival_function = np.array(survival_function)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_survival_function(self, times):\n",
    "        \"\"\"Predict survival probability at given times\"\"\"\n",
    "        if self.survival_function is None:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "        \n",
    "        # Interpolate survival function\n",
    "        return np.interp(times, self.timeline, self.survival_function)\n",
    "\n",
    "# Fit overall survival curve\n",
    "km_overall = KaplanMeierEstimator()\n",
    "km_overall.fit(survival_data['duration_years'].values, survival_data['event'].values)\n",
    "\n",
    "# Fit by high/low royalty rate\n",
    "high_royalty_mask = survival_data['high_royalty'] == 1\n",
    "low_royalty_mask = survival_data['high_royalty'] == 0\n",
    "\n",
    "km_high_royalty = KaplanMeierEstimator()\n",
    "km_high_royalty.fit(\n",
    "    survival_data.loc[high_royalty_mask, 'duration_years'].values,\n",
    "    survival_data.loc[high_royalty_mask, 'event'].values\n",
    ")\n",
    "\n",
    "km_low_royalty = KaplanMeierEstimator()\n",
    "km_low_royalty.fit(\n",
    "    survival_data.loc[low_royalty_mask, 'duration_years'].values,\n",
    "    survival_data.loc[low_royalty_mask, 'event'].values\n",
    ")\n",
    "\n",
    "print(\"Kaplan-Meier survival curves fitted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize survival curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Survival Analysis: Alaska OCS Lease Duration', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Overall survival curve\n",
    "axes[0,0].step(km_overall.timeline, km_overall.survival_function, where='post', linewidth=2, color='blue')\n",
    "axes[0,0].set_title('Overall Lease Survival Function')\n",
    "axes[0,0].set_xlabel('Duration (Years)')\n",
    "axes[0,0].set_ylabel('Survival Probability')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# Survival by royalty rate\n",
    "axes[0,1].step(km_high_royalty.timeline, km_high_royalty.survival_function, \n",
    "               where='post', linewidth=2, color='red', label='High Royalty Rate')\n",
    "axes[0,1].step(km_low_royalty.timeline, km_low_royalty.survival_function, \n",
    "               where='post', linewidth=2, color='green', label='Low Royalty Rate')\n",
    "axes[0,1].set_title('Survival by Royalty Rate')\n",
    "axes[0,1].set_xlabel('Duration (Years)')\n",
    "axes[0,1].set_ylabel('Survival Probability')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_ylim(0, 1)\n",
    "\n",
    "# Duration histogram by event status\n",
    "terminated_durations = survival_data[survival_data['event'] == 1]['duration_years']\n",
    "censored_durations = survival_data[survival_data['event'] == 0]['duration_years']\n",
    "\n",
    "axes[1,0].hist(terminated_durations, bins=30, alpha=0.7, color='red', \n",
    "               label=f'Terminated (n={len(terminated_durations)})', density=True)\n",
    "axes[1,0].hist(censored_durations, bins=30, alpha=0.7, color='green', \n",
    "               label=f'Censored (n={len(censored_durations)})', density=True)\n",
    "axes[1,0].set_title('Duration Distribution by Status')\n",
    "axes[1,0].set_xlabel('Duration (Years)')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Hazard rate estimation (rough approximation)\n",
    "time_bins = np.linspace(0, survival_data['duration_years'].max(), 20)\n",
    "hazard_rates = []\n",
    "\n",
    "for i in range(len(time_bins)-1):\n",
    "    t1, t2 = time_bins[i], time_bins[i+1]\n",
    "    in_interval = (survival_data['duration_years'] >= t1) & (survival_data['duration_years'] < t2)\n",
    "    at_risk = (survival_data['duration_years'] >= t1).sum()\n",
    "    events = (in_interval & (survival_data['event'] == 1)).sum()\n",
    "    \n",
    "    if at_risk > 0:\n",
    "        hazard_rates.append(events / (at_risk * (t2 - t1)))\n",
    "    else:\n",
    "        hazard_rates.append(0)\n",
    "\n",
    "bin_centers = (time_bins[:-1] + time_bins[1:]) / 2\n",
    "axes[1,1].plot(bin_centers, hazard_rates, 'o-', linewidth=2, color='purple')\n",
    "axes[1,1].set_title('Estimated Hazard Rate')\n",
    "axes[1,1].set_xlabel('Duration (Years)')\n",
    "axes[1,1].set_ylabel('Hazard Rate')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate survival statistics\n",
    "median_survival_time = None\n",
    "for i, s in enumerate(km_overall.survival_function):\n",
    "    if s <= 0.5:\n",
    "        median_survival_time = km_overall.timeline[i]\n",
    "        break\n",
    "\n",
    "survival_5y = km_overall.predict_survival_function([5])[0]\n",
    "survival_10y = km_overall.predict_survival_function([10])[0]\n",
    "\n",
    "print(\"\\nSurvival Analysis Results:\")\n",
    "print(\"=\" * 35)\n",
    "if median_survival_time:\n",
    "    print(f\"Median survival time: {median_survival_time:.1f} years\")\n",
    "else:\n",
    "    print(\"Median survival time: Not reached (>50% still active)\")\n",
    "print(f\"5-year survival probability: {survival_5y:.2%}\")\n",
    "print(f\"10-year survival probability: {survival_10y:.2%}\")\n",
    "print(f\"Mean duration (observed): {survival_data['duration_years'].mean():.1f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Causal Inference: Policy Impact Analysis\n",
    "\n",
    "Using propensity score matching to estimate the causal effect of high royalty rates on lease termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare causal inference analysis\n",
    "# Treatment: High royalty rate (above median)\n",
    "# Outcome: Lease termination within 10 years\n",
    "\n",
    "causal_data = survival_data[\n",
    "    (survival_data['duration_years'] > 0) &\n",
    "    (survival_data['ROYALTY_RATE'].notna()) &\n",
    "    (survival_data['CURRENT_AREA'] > 0) &\n",
    "    (survival_data['SALE_YEAR'] >= 1995)  # Focus on more recent period\n",
    "].copy()\n",
    "\n",
    "# Define treatment and outcome\n",
    "treatment = causal_data['high_royalty'].values\n",
    "outcome = ((causal_data['duration_years'] <= 10) & (causal_data['event'] == 1)).astype(int).values\n",
    "\n",
    "# Confounding variables for propensity score\n",
    "confounders = ['log_area', 'PRIMARY_TERM', 'SALE_YEAR']\n",
    "X_confounders = causal_data[confounders].values\n",
    "\n",
    "# Standardize confounders\n",
    "scaler = StandardScaler()\n",
    "X_confounders_scaled = scaler.fit_transform(X_confounders)\n",
    "\n",
    "print(f\"Causal analysis dataset: {len(causal_data)} observations\")\n",
    "print(f\"Treatment groups:\")\n",
    "print(f\"  High royalty: {treatment.sum()} ({treatment.mean():.1%})\")\n",
    "print(f\"  Low royalty: {(1-treatment).sum()} ({(1-treatment).mean():.1%})\")\n",
    "print(f\"Outcome (termination within 10 years): {outcome.sum()} ({outcome.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate propensity scores\n",
    "propensity_model = LogisticRegression(random_state=42)\n",
    "propensity_model.fit(X_confounders_scaled, treatment)\n",
    "propensity_scores = propensity_model.predict_proba(X_confounders_scaled)[:, 1]\n",
    "\n",
    "# Check propensity score overlap\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(propensity_scores[treatment == 0], bins=20, alpha=0.7, color='blue', \n",
    "         label='Low Royalty', density=True)\n",
    "plt.hist(propensity_scores[treatment == 1], bins=20, alpha=0.7, color='red', \n",
    "         label='High Royalty', density=True)\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Propensity Score Distributions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Check covariate balance before matching\n",
    "def calculate_standardized_mean_difference(X, treatment):\n",
    "    \"\"\"Calculate standardized mean differences\"\"\"\n",
    "    treated_means = X[treatment == 1].mean(axis=0)\n",
    "    control_means = X[treatment == 0].mean(axis=0)\n",
    "    pooled_std = np.sqrt((X[treatment == 1].var(axis=0) + X[treatment == 0].var(axis=0)) / 2)\n",
    "    return (treated_means - control_means) / pooled_std\n",
    "\n",
    "smd_before = calculate_standardized_mean_difference(X_confounders_scaled, treatment)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(confounders, smd_before, color=['red' if abs(x) > 0.1 else 'green' for x in smd_before])\n",
    "plt.axvline(0.1, color='red', linestyle='--', alpha=0.7, label='|SMD| = 0.1')\n",
    "plt.axvline(-0.1, color='red', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Standardized Mean Difference')\n",
    "plt.title('Covariate Balance (Before Matching)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "print(\"Propensity score model coefficients:\")\n",
    "for feature, coef in zip(confounders, propensity_model.coef_[0]):\n",
    "    print(f\"  {feature}: {coef:.3f}\")\n",
    "\n",
    "print(f\"\\nPropensity score overlap:\")\n",
    "print(f\"  Min in treated: {propensity_scores[treatment == 1].min():.3f}\")\n",
    "print(f\"  Max in control: {propensity_scores[treatment == 0].max():.3f}\")\n",
    "print(f\"  Common support: {propensity_scores.min():.3f} - {propensity_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propensity score matching\n",
    "def propensity_score_matching(propensity_scores, treatment, outcome, X_confounders, n_neighbors=1):\n",
    "    \"\"\"Perform propensity score matching\"\"\"\n",
    "    treated_indices = np.where(treatment == 1)[0]\n",
    "    control_indices = np.where(treatment == 0)[0]\n",
    "    \n",
    "    # Fit nearest neighbors on control group\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "    nn.fit(propensity_scores[control_indices].reshape(-1, 1))\n",
    "    \n",
    "    # Find matches for treated units\n",
    "    distances, match_indices = nn.kneighbors(propensity_scores[treated_indices].reshape(-1, 1))\n",
    "    \n",
    "    matched_control_indices = control_indices[match_indices.flatten()]\n",
    "    matched_treated_indices = treated_indices\n",
    "    \n",
    "    # Create matched dataset\n",
    "    matched_indices = np.concatenate([matched_treated_indices, matched_control_indices])\n",
    "    matched_treatment = np.concatenate([np.ones(len(matched_treated_indices)), \n",
    "                                       np.zeros(len(matched_control_indices))])\n",
    "    matched_outcome = outcome[matched_indices]\n",
    "    matched_confounders = X_confounders[matched_indices]\n",
    "    matched_propensity = propensity_scores[matched_indices]\n",
    "    \n",
    "    return (matched_indices, matched_treatment.astype(int), matched_outcome, \n",
    "            matched_confounders, matched_propensity, distances.flatten())\n",
    "\n",
    "# Perform matching\n",
    "(matched_indices, matched_treatment, matched_outcome, \n",
    " matched_confounders, matched_propensity, match_distances) = propensity_score_matching(\n",
    "    propensity_scores, treatment, outcome, X_confounders_scaled\n",
    ")\n",
    "\n",
    "# Check post-matching balance\n",
    "smd_after = calculate_standardized_mean_difference(matched_confounders, matched_treatment)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(confounders, smd_after, color=['red' if abs(x) > 0.1 else 'green' for x in smd_after])\n",
    "plt.axvline(0.1, color='red', linestyle='--', alpha=0.7, label='|SMD| = 0.1')\n",
    "plt.axvline(-0.1, color='red', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Standardized Mean Difference')\n",
    "plt.title('Covariate Balance (After Matching)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMatching Results:\")\n",
    "print(f\"Original sample size: {len(treatment)}\")\n",
    "print(f\"Matched sample size: {len(matched_treatment)}\")\n",
    "print(f\"Average matching distance: {match_distances.mean():.4f}\")\n",
    "print(f\"Max matching distance: {match_distances.max():.4f}\")\n",
    "\n",
    "print(\"\\nStandardized Mean Differences:\")\n",
    "print(\"Variable\\t\\tBefore\\tAfter\\tImprovement\")\n",
    "print(\"-\" * 50)\n",
    "for i, var in enumerate(confounders):\n",
    "    improvement = abs(smd_before[i]) - abs(smd_after[i])\n",
    "    print(f\"{var:<15}\\t{smd_before[i]:6.3f}\\t{smd_after[i]:6.3f}\\t{improvement:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate treatment effects\n",
    "def calculate_treatment_effects(matched_treatment, matched_outcome):\n",
    "    \"\"\"Calculate various treatment effect estimates\"\"\"\n",
    "    treated_outcomes = matched_outcome[matched_treatment == 1]\n",
    "    control_outcomes = matched_outcome[matched_treatment == 0]\n",
    "    \n",
    "    # Average Treatment Effect (ATE)\n",
    "    ate = treated_outcomes.mean() - control_outcomes.mean()\n",
    "    \n",
    "    # Standard error using simple difference of proportions\n",
    "    n_treated = len(treated_outcomes)\n",
    "    n_control = len(control_outcomes)\n",
    "    \n",
    "    p_treated = treated_outcomes.mean()\n",
    "    p_control = control_outcomes.mean()\n",
    "    \n",
    "    se_ate = np.sqrt(p_treated * (1 - p_treated) / n_treated + \n",
    "                     p_control * (1 - p_control) / n_control)\n",
    "    \n",
    "    # Confidence interval\n",
    "    ci_lower = ate - 1.96 * se_ate\n",
    "    ci_upper = ate + 1.96 * se_ate\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat = ate / se_ate if se_ate > 0 else np.inf\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(t_stat)))\n",
    "    \n",
    "    return {\n",
    "        'ate': ate,\n",
    "        'se': se_ate,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'n_treated': n_treated,\n",
    "        'n_control': n_control,\n",
    "        'treated_rate': p_treated,\n",
    "        'control_rate': p_control\n",
    "    }\n",
    "\n",
    "# Calculate effects for matched sample\n",
    "matched_effects = calculate_treatment_effects(matched_treatment, matched_outcome)\n",
    "\n",
    "# Calculate naive estimate (without matching)\n",
    "naive_effects = calculate_treatment_effects(treatment, outcome)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Treatment effect comparison\n",
    "methods = ['Naive\\n(Unmatched)', 'Propensity Score\\nMatching']\n",
    "effects = [naive_effects['ate'], matched_effects['ate']]\n",
    "errors = [naive_effects['se'] * 1.96, matched_effects['se'] * 1.96]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = axes[0].bar(methods, effects, yerr=errors, capsize=10, color=colors, \n",
    "                   alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0].set_ylabel('Average Treatment Effect')\n",
    "axes[0].set_title('Treatment Effect Estimates\\n(High vs Low Royalty Rates)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, effect, error in zip(bars, effects, errors):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + error + 0.01,\n",
    "                f'{effect:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Outcome rates by treatment group\n",
    "treatment_groups = ['Low Royalty', 'High Royalty']\n",
    "matched_rates = [matched_effects['control_rate'], matched_effects['treated_rate']]\n",
    "naive_rates = [naive_effects['control_rate'], naive_effects['treated_rate']]\n",
    "\n",
    "x = np.arange(len(treatment_groups))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, naive_rates, width, label='Naive', \n",
    "                    color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "bars2 = axes[1].bar(x + width/2, matched_rates, width, label='Matched', \n",
    "                    color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "\n",
    "axes[1].set_xlabel('Treatment Group')\n",
    "axes[1].set_ylabel('Termination Rate (within 10 years)')\n",
    "axes[1].set_title('Outcome Rates by Treatment Group')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(treatment_groups)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                    f'{height:.2%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nCausal Inference Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nNaive Estimate (Without Matching):\")\n",
    "print(f\"  ATE: {naive_effects['ate']:.4f} (95% CI: {naive_effects['ci_lower']:.4f}, {naive_effects['ci_upper']:.4f})\")\n",
    "print(f\"  P-value: {naive_effects['p_value']:.4f}\")\n",
    "print(f\"  Control rate: {naive_effects['control_rate']:.2%}\")\n",
    "print(f\"  Treated rate: {naive_effects['treated_rate']:.2%}\")\n",
    "\n",
    "print(\"\\nPropensity Score Matching Estimate:\")\n",
    "print(f\"  ATE: {matched_effects['ate']:.4f} (95% CI: {matched_effects['ci_lower']:.4f}, {matched_effects['ci_upper']:.4f})\")\n",
    "print(f\"  P-value: {matched_effects['p_value']:.4f}\")\n",
    "print(f\"  Control rate: {matched_effects['control_rate']:.2%}\")\n",
    "print(f\"  Treated rate: {matched_effects['treated_rate']:.2%}\")\n",
    "\n",
    "significance = \"significant\" if matched_effects['p_value'] < 0.05 else \"not significant\"\n",
    "direction = \"increases\" if matched_effects['ate'] > 0 else \"decreases\"\n",
    "\n",
    "print(f\"\\nConclusion:\")\n",
    "print(f\"High royalty rates {direction} the probability of lease termination\")\n",
    "print(f\"within 10 years by {abs(matched_effects['ate']):.1%} (statistically {significance}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Advanced Statistical Methods\n",
    "\n",
    "This notebook demonstrates sophisticated statistical techniques that go beyond traditional machine learning:\n",
    "\n",
    "### 1. **Bayesian Regression**\n",
    "- Provides uncertainty quantification for predictions\n",
    "- Enables probabilistic interpretation of coefficients\n",
    "- Essential for financial decision-making under uncertainty\n",
    "\n",
    "### 2. **Time Series Forecasting**\n",
    "- ARIMA modeling captures temporal dependencies\n",
    "- Enables prediction of future lease activity patterns\n",
    "- Critical for resource planning and market timing\n",
    "\n",
    "### 3. **Survival Analysis**\n",
    "- Models time-to-event (lease termination) data\n",
    "- Handles censoring naturally\n",
    "- Provides insights into lease longevity factors\n",
    "\n",
    "### 4. **Causal Inference**\n",
    "- Estimates causal effects using propensity score matching\n",
    "- Controls for confounding variables\n",
    "- Enables policy evaluation and decision support\n",
    "\n",
    "These methods showcase advanced statistical thinking and provide actionable insights for:\n",
    "- **Investment decisions** with quantified uncertainty\n",
    "- **Market timing** through time series forecasting\n",
    "- **Risk assessment** via survival analysis\n",
    "- **Policy evaluation** through causal inference\n",
    "\n",
    "The combination of these techniques demonstrates sophisticated data science capabilities suitable for senior-level positions in finance, consulting, and policy analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}