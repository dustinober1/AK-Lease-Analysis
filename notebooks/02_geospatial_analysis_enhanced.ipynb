{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alaska Oil & Gas Lease Analysis - Enhanced Geospatial Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive geospatial analysis of Alaska OCS lease data with advanced spatial statistics, confidence intervals for spatial patterns, and rigorous statistical validation of geographic trends.\n",
    "\n",
    "## Technical Approach\n",
    "- **Spatial Statistics**: Moran's I for spatial autocorrelation\n",
    "- **Hot Spot Analysis**: Getis-Ord Gi* statistic with significance testing\n",
    "- **Distance Analysis**: Nearest neighbor analysis with confidence intervals\n",
    "- **Kernel Density**: Bandwidth optimization and statistical validation\n",
    "- **Spatial Clustering**: DBSCAN with parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"Geospatial libraries imported successfully\")\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"Folium available: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Spatial Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV and GeoJSON data\n",
    "df = pd.read_csv('../data/AK_Leases.csv')\n",
    "gdf = gpd.read_file('../data/AK_Leases.geojson')\n",
    "\n",
    "print(f\"CSV data shape: {df.shape}\")\n",
    "print(f\"GeoJSON data shape: {gdf.shape}\")\n",
    "print(f\"Coordinate Reference System: {gdf.crs}\")\n",
    "\n",
    "# Merge datasets if they have different information\n",
    "if 'LEASE_NUMBER' in df.columns and 'LEASE_NUMBER' in gdf.columns:\n",
    "    # Merge on lease number\n",
    "    merged_gdf = gdf.merge(df[['LEASE_NUMBER', 'BID_AMOUNT', 'CURRENT_AREA', 'ROYALTY_RATE', 'LEASE_IS_ACTIVE']], \n",
    "                          on='LEASE_NUMBER', how='left')\n",
    "    print(f\"Merged data shape: {merged_gdf.shape}\")\n",
    "else:\n",
    "    merged_gdf = gdf.copy()\n",
    "    print(\"Using GeoJSON data directly\")\n",
    "\n",
    "# Extract centroids for point analysis\n",
    "merged_gdf['centroid'] = merged_gdf.geometry.centroid\n",
    "merged_gdf['longitude'] = merged_gdf.centroid.x\n",
    "merged_gdf['latitude'] = merged_gdf.centroid.y\n",
    "\n",
    "# Clean data for analysis\n",
    "spatial_df = merged_gdf.dropna(subset=['longitude', 'latitude']).copy()\n",
    "print(f\"\\nSpatial analysis dataset: {len(spatial_df):,} leases with coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spatial Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_morans_i(x, y, values, distance_threshold=None):\n",
    "    \"\"\"Calculate Moran's I spatial autocorrelation statistic with significance testing\"\"\"\n",
    "    n = len(values)\n",
    "    \n",
    "    # Calculate distance matrix\n",
    "    coords = np.column_stack([x, y])\n",
    "    distances = squareform(pdist(coords))\n",
    "    \n",
    "    # Create spatial weights matrix (inverse distance or binary)\n",
    "    if distance_threshold is None:\n",
    "        # Use inverse distance weights\n",
    "        weights = 1 / (distances + 1e-10)  # Add small value to avoid division by zero\n",
    "        np.fill_diagonal(weights, 0)  # No self-weights\n",
    "    else:\n",
    "        # Binary weights within threshold\n",
    "        weights = (distances <= distance_threshold).astype(float)\n",
    "        np.fill_diagonal(weights, 0)\n",
    "    \n",
    "    # Normalize weights\n",
    "    row_sums = weights.sum(axis=1)\n",
    "    weights = weights / row_sums[:, np.newaxis]\n",
    "    weights[np.isnan(weights)] = 0\n",
    "    \n",
    "    # Calculate Moran's I\n",
    "    values_centered = values - np.mean(values)\n",
    "    numerator = np.sum(weights * np.outer(values_centered, values_centered))\n",
    "    denominator = np.sum(values_centered**2)\n",
    "    \n",
    "    morans_i = (n / np.sum(weights)) * (numerator / denominator)\n",
    "    \n",
    "    # Expected value and variance under null hypothesis\n",
    "    expected_i = -1 / (n - 1)\n",
    "    \n",
    "    # Simplified variance calculation\n",
    "    S0 = np.sum(weights)\n",
    "    S1 = 0.5 * np.sum((weights + weights.T)**2)\n",
    "    S2 = np.sum(np.sum(weights + weights.T, axis=1)**2)\n",
    "    \n",
    "    b2 = n * np.sum(values_centered**4) / (np.sum(values_centered**2)**2)\n",
    "    \n",
    "    variance_i = ((n*((n**2 - 3*n + 3)*S1 - n*S2 + 3*S0**2) - \n",
    "                   b2*((n**2 - n)*S1 - 2*n*S2 + 6*S0**2)) / \n",
    "                  ((n-1)*(n-2)*(n-3)*S0**2)) - expected_i**2\n",
    "    \n",
    "    # Z-score and p-value\n",
    "    if variance_i > 0:\n",
    "        z_score = (morans_i - expected_i) / np.sqrt(variance_i)\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "    else:\n",
    "        z_score = np.nan\n",
    "        p_value = np.nan\n",
    "    \n",
    "    return {\n",
    "        'morans_i': morans_i,\n",
    "        'expected_i': expected_i,\n",
    "        'variance_i': variance_i,\n",
    "        'z_score': z_score,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "# Test spatial autocorrelation for different variables\n",
    "if 'BID_AMOUNT' in spatial_df.columns:\n",
    "    # Filter out missing bid amounts\n",
    "    bid_spatial = spatial_df.dropna(subset=['BID_AMOUNT'])\n",
    "    bid_spatial = bid_spatial[bid_spatial['BID_AMOUNT'] > 0]\n",
    "    \n",
    "    if len(bid_spatial) > 10:\n",
    "        # Log transform bid amounts\n",
    "        log_bids = np.log10(bid_spatial['BID_AMOUNT'])\n",
    "        \n",
    "        bid_autocorr = calculate_morans_i(\n",
    "            bid_spatial['longitude'], \n",
    "            bid_spatial['latitude'], \n",
    "            log_bids\n",
    "        )\n",
    "        \n",
    "        print(\"=== Spatial Autocorrelation Analysis ===\")\n",
    "        print(f\"Log10(Bid Amount) Moran's I: {bid_autocorr['morans_i']:.4f}\")\n",
    "        print(f\"Expected under null: {bid_autocorr['expected_i']:.4f}\")\n",
    "        print(f\"Z-score: {bid_autocorr['z_score']:.4f}\")\n",
    "        print(f\"P-value: {bid_autocorr['p_value']:.4f}\")\n",
    "        \n",
    "        if bid_autocorr['p_value'] < 0.05:\n",
    "            interpretation = \"Significant spatial autocorrelation detected\"\n",
    "            if bid_autocorr['morans_i'] > bid_autocorr['expected_i']:\n",
    "                interpretation += \" (clustered pattern)\"\n",
    "            else:\n",
    "                interpretation += \" (dispersed pattern)\"\n",
    "        else:\n",
    "            interpretation = \"No significant spatial autocorrelation (random pattern)\"\n",
    "        \n",
    "        print(f\"Interpretation: {interpretation}\")\n",
    "    else:\n",
    "        print(\"Insufficient data for spatial autocorrelation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hot Spot Analysis with Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getis_ord_gi_star(x, y, values, distance_threshold_km=50):\n",
    "    \"\"\"Calculate Getis-Ord Gi* hot spot statistic with significance testing\"\"\"\n",
    "    n = len(values)\n",
    "    coords = np.column_stack([x, y])\n",
    "    \n",
    "    # Convert distance threshold from km to degrees (approximate)\n",
    "    distance_threshold = distance_threshold_km / 111.0  # Rough conversion\n",
    "    \n",
    "    # Calculate distances\n",
    "    distances = squareform(pdist(coords))\n",
    "    \n",
    "    gi_stats = []\n",
    "    p_values = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Create weights for neighbors within threshold\n",
    "        weights = (distances[i] <= distance_threshold).astype(float)\n",
    "        \n",
    "        # Calculate Gi* statistic\n",
    "        if np.sum(weights) > 1:  # Need at least one neighbor\n",
    "            weighted_sum = np.sum(weights * values)\n",
    "            sum_weights = np.sum(weights)\n",
    "            \n",
    "            # Mean and variance calculations\n",
    "            mean_val = np.mean(values)\n",
    "            var_val = np.var(values)\n",
    "            \n",
    "            # Expected value and variance of Gi*\n",
    "            expected_gi = sum_weights * mean_val\n",
    "            variance_gi = (sum_weights * (n - sum_weights) * var_val) / (n - 1)\n",
    "            \n",
    "            if variance_gi > 0:\n",
    "                gi_star = (weighted_sum - expected_gi) / np.sqrt(variance_gi)\n",
    "                p_val = 2 * (1 - stats.norm.cdf(abs(gi_star)))\n",
    "            else:\n",
    "                gi_star = 0\n",
    "                p_val = 1.0\n",
    "        else:\n",
    "            gi_star = 0\n",
    "            p_val = 1.0\n",
    "        \n",
    "        gi_stats.append(gi_star)\n",
    "        p_values.append(p_val)\n",
    "    \n",
    "    return np.array(gi_stats), np.array(p_values)\n",
    "\n",
    "# Perform hot spot analysis\n",
    "if 'BID_AMOUNT' in spatial_df.columns and len(bid_spatial) > 10:\n",
    "    gi_stats, gi_p_values = getis_ord_gi_star(\n",
    "        bid_spatial['longitude'],\n",
    "        bid_spatial['latitude'],\n",
    "        log_bids,\n",
    "        distance_threshold_km=50\n",
    "    )\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    bid_spatial = bid_spatial.copy()\n",
    "    bid_spatial['gi_star'] = gi_stats\n",
    "    bid_spatial['gi_p_value'] = gi_p_values\n",
    "    \n",
    "    # Classify hot spots and cold spots\n",
    "    alpha = 0.05\n",
    "    bid_spatial['hotspot_type'] = 'Not Significant'\n",
    "    bid_spatial.loc[(gi_stats > 1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Hot Spot (99%)'\n",
    "    bid_spatial.loc[(gi_stats > 1.65) & (gi_stats <= 1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Hot Spot (95%)'\n",
    "    bid_spatial.loc[(gi_stats < -1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Cold Spot (99%)'\n",
    "    bid_spatial.loc[(gi_stats < -1.65) & (gi_stats >= -1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Cold Spot (95%)'\n",
    "    \n",
    "    print(\"\\n=== Hot Spot Analysis Results ===\")\n",
    "    hotspot_counts = bid_spatial['hotspot_type'].value_counts()\n",
    "    print(hotspot_counts)\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(f\"\\nGi* Statistics Summary:\")\n",
    "    print(f\"Mean Gi*: {gi_stats.mean():.4f}\")\n",
    "    print(f\"Std Gi*: {gi_stats.std():.4f}\")\n",
    "    print(f\"Max Gi*: {gi_stats.max():.4f}\")\n",
    "    print(f\"Min Gi*: {gi_stats.min():.4f}\")\n",
    "    print(f\"Significant hot/cold spots: {np.sum(gi_p_values < alpha)} ({100*np.sum(gi_p_values < alpha)/len(gi_p_values):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Clustering with Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dbscan_parameters(coords, eps_range=None, min_samples_range=None):\n",
    "    \"\"\"Optimize DBSCAN parameters using silhouette score\"\"\"\n",
    "    if eps_range is None:\n",
    "        # Calculate reasonable eps range based on k-distance\n",
    "        k = 4\n",
    "        nbrs = NearestNeighbors(n_neighbors=k).fit(coords)\n",
    "        distances, indices = nbrs.kneighbors(coords)\n",
    "        k_distances = np.sort(distances[:, k-1])\n",
    "        \n",
    "        eps_range = np.linspace(k_distances[len(k_distances)//4], \n",
    "                               k_distances[3*len(k_distances)//4], 10)\n",
    "    \n",
    "    if min_samples_range is None:\n",
    "        min_samples_range = range(3, min(15, len(coords)//10))\n",
    "    \n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(coords)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "            \n",
    "            if n_clusters > 1 and n_clusters < len(coords) - 1:\n",
    "                score = silhouette_score(coords, labels)\n",
    "                \n",
    "                results.append({\n",
    "                    'eps': eps,\n",
    "                    'min_samples': min_samples,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'n_noise': n_noise,\n",
    "                    'silhouette_score': score\n",
    "                })\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "    \n",
    "    return best_params, results\n",
    "\n",
    "# Optimize DBSCAN clustering\n",
    "if len(spatial_df) > 20:\n",
    "    # Prepare coordinates\n",
    "    coords = spatial_df[['longitude', 'latitude']].values\n",
    "    \n",
    "    # Standardize coordinates for clustering\n",
    "    scaler = StandardScaler()\n",
    "    coords_scaled = scaler.fit_transform(coords)\n",
    "    \n",
    "    print(\"Optimizing DBSCAN parameters...\")\n",
    "    best_params, optimization_results = optimize_dbscan_parameters(coords_scaled)\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"\\nOptimal DBSCAN parameters:\")\n",
    "        print(f\"eps: {best_params['eps']:.4f}\")\n",
    "        print(f\"min_samples: {best_params['min_samples']}\")\n",
    "        \n",
    "        # Apply optimal clustering\n",
    "        optimal_dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
    "        cluster_labels = optimal_dbscan.fit_predict(coords_scaled)\n",
    "        \n",
    "        # Add cluster labels to dataframe\n",
    "        spatial_df = spatial_df.copy()\n",
    "        spatial_df['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analyze clusters\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        \n",
    "        print(f\"\\n=== Spatial Clustering Results ===\")\n",
    "        print(f\"Number of clusters: {n_clusters}\")\n",
    "        print(f\"Number of noise points: {n_noise}\")\n",
    "        print(f\"Clustered points: {len(spatial_df) - n_noise} ({100*(len(spatial_df) - n_noise)/len(spatial_df):.1f}%)\")\n",
    "        \n",
    "        # Cluster statistics\n",
    "        if 'BID_AMOUNT' in spatial_df.columns:\n",
    "            cluster_stats = spatial_df[spatial_df['cluster'] != -1].groupby('cluster').agg({\n",
    "                'BID_AMOUNT': ['count', 'mean', 'median'],\n",
    "                'longitude': ['mean'],\n",
    "                'latitude': ['mean']\n",
    "            }).round(4)\n",
    "            \n",
    "            print(f\"\\nCluster Statistics:\")\n",
    "            print(cluster_stats)\n",
    "    else:\n",
    "        print(\"Could not find optimal clustering parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive spatial visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# 1. Basic lease distribution\n",
    "axes[0, 0].scatter(spatial_df['longitude'], spatial_df['latitude'], \n",
    "                   alpha=0.6, s=30, c='blue')\n",
    "axes[0, 0].set_xlabel('Longitude')\n",
    "axes[0, 0].set_ylabel('Latitude')\n",
    "axes[0, 0].set_title(f'Lease Locations (n={len(spatial_df):,})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Bid amount visualization (if available)\n",
    "if 'BID_AMOUNT' in spatial_df.columns and len(bid_spatial) > 0:\n",
    "    scatter = axes[0, 1].scatter(bid_spatial['longitude'], bid_spatial['latitude'], \n",
    "                                c=np.log10(bid_spatial['BID_AMOUNT']), \n",
    "                                s=50, alpha=0.7, cmap='viridis')\n",
    "    plt.colorbar(scatter, ax=axes[0, 1], label='Log10(Bid Amount)')\n",
    "    axes[0, 1].set_xlabel('Longitude')\n",
    "    axes[0, 1].set_ylabel('Latitude')\n",
    "    axes[0, 1].set_title('Lease Locations by Bid Amount')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Bid Amount Data\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Bid Amount Analysis')\n",
    "\n",
    "# 3. Hot spot analysis (if performed)\n",
    "if 'gi_star' in locals() and len(bid_spatial) > 0:\n",
    "    # Create color map for hot spots\n",
    "    colors = []\n",
    "    for hs_type in bid_spatial['hotspot_type']:\n",
    "        if 'Hot Spot (99%)' in hs_type:\n",
    "            colors.append('red')\n",
    "        elif 'Hot Spot (95%)' in hs_type:\n",
    "            colors.append('orange')\n",
    "        elif 'Cold Spot (99%)' in hs_type:\n",
    "            colors.append('blue')\n",
    "        elif 'Cold Spot (95%)' in hs_type:\n",
    "            colors.append('lightblue')\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "    \n",
    "    axes[1, 0].scatter(bid_spatial['longitude'], bid_spatial['latitude'], \n",
    "                       c=colors, s=50, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Longitude')\n",
    "    axes[1, 0].set_ylabel('Latitude')\n",
    "    axes[1, 0].set_title('Hot Spot Analysis (Getis-Ord Gi*)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    unique_types = bid_spatial['hotspot_type'].unique()\n",
    "    legend_elements = []\n",
    "    for hs_type in unique_types:\n",
    "        if 'Hot Spot (99%)' in hs_type:\n",
    "            color = 'red'\n",
    "        elif 'Hot Spot (95%)' in hs_type:\n",
    "            color = 'orange'\n",
    "        elif 'Cold Spot (99%)' in hs_type:\n",
    "            color = 'blue'\n",
    "        elif 'Cold Spot (95%)' in hs_type:\n",
    "            color = 'lightblue'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                         markerfacecolor=color, markersize=8, label=hs_type))\n",
    "    \n",
    "    axes[1, 0].legend(handles=legend_elements, loc='best', fontsize=8)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Hot Spot Analysis\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Hot Spot Analysis')\n",
    "\n",
    "# 4. Spatial clustering (if performed)\n",
    "if 'cluster' in spatial_df.columns:\n",
    "    # Color by cluster\n",
    "    unique_clusters = spatial_df['cluster'].unique()\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_clusters)))\n",
    "    \n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        mask = spatial_df['cluster'] == cluster\n",
    "        if cluster == -1:\n",
    "            # Noise points\n",
    "            axes[1, 1].scatter(spatial_df.loc[mask, 'longitude'], \n",
    "                              spatial_df.loc[mask, 'latitude'], \n",
    "                              c='black', s=20, alpha=0.5, label='Noise')\n",
    "        else:\n",
    "            axes[1, 1].scatter(spatial_df.loc[mask, 'longitude'], \n",
    "                              spatial_df.loc[mask, 'latitude'], \n",
    "                              c=[colors[i]], s=50, alpha=0.7, \n",
    "                              label=f'Cluster {cluster}')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Longitude')\n",
    "    axes[1, 1].set_ylabel('Latitude')\n",
    "    axes[1, 1].set_title('Spatial Clusters (DBSCAN)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].legend(loc='best', fontsize=8)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Spatial Clustering\\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Spatial Clustering')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary and Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spatial_confidence_intervals(x, y, confidence=0.95):\n",
    "    \"\"\"Calculate confidence intervals for spatial center and dispersion\"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Mean center\n",
    "    mean_x = np.mean(x)\n",
    "    mean_y = np.mean(y)\n",
    "    \n",
    "    # Standard errors\n",
    "    se_x = np.std(x) / np.sqrt(n)\n",
    "    se_y = np.std(y) / np.sqrt(n)\n",
    "    \n",
    "    # Confidence intervals for center\n",
    "    alpha = 1 - confidence\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, n - 1)\n",
    "    \n",
    "    ci_x = (mean_x - t_critical * se_x, mean_x + t_critical * se_x)\n",
    "    ci_y = (mean_y - t_critical * se_y, mean_y + t_critical * se_y)\n",
    "    \n",
    "    # Standard distance (measure of dispersion)\n",
    "    std_distance = np.sqrt(np.mean((x - mean_x)**2 + (y - mean_y)**2))\n",
    "    \n",
    "    # Confidence interval for standard distance\n",
    "    distances = np.sqrt((x - mean_x)**2 + (y - mean_y)**2)\n",
    "    se_std_dist = np.std(distances) / np.sqrt(n)\n",
    "    ci_std_dist = (std_distance - t_critical * se_std_dist, \n",
    "                   std_distance + t_critical * se_std_dist)\n",
    "    \n",
    "    return {\n",
    "        'mean_center': (mean_x, mean_y),\n",
    "        'ci_center_x': ci_x,\n",
    "        'ci_center_y': ci_y,\n",
    "        'standard_distance': std_distance,\n",
    "        'ci_standard_distance': ci_std_dist,\n",
    "        'n': n\n",
    "    }\n",
    "\n",
    "# Calculate spatial statistics with confidence intervals\n",
    "spatial_stats = calculate_spatial_confidence_intervals(\n",
    "    spatial_df['longitude'], \n",
    "    spatial_df['latitude']\n",
    ")\n",
    "\n",
    "print(\"=== COMPREHENSIVE GEOSPATIAL ANALYSIS SUMMARY ===\")\n",
    "print(f\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   • Total lease locations: {len(spatial_df):,}\")\n",
    "print(f\"   • Coordinate system: {gdf.crs if hasattr(gdf, 'crs') else 'Not specified'}\")\n",
    "print(f\"   • Longitude range: {spatial_df['longitude'].min():.4f} to {spatial_df['longitude'].max():.4f}\")\n",
    "print(f\"   • Latitude range: {spatial_df['latitude'].min():.4f} to {spatial_df['latitude'].max():.4f}\")\n",
    "\n",
    "print(f\"\\n2. SPATIAL CENTER AND DISPERSION (95% CI):\")\n",
    "print(f\"   • Mean center: ({spatial_stats['mean_center'][0]:.4f}, {spatial_stats['mean_center'][1]:.4f})\")\n",
    "print(f\"   • Longitude CI: [{spatial_stats['ci_center_x'][0]:.4f}, {spatial_stats['ci_center_x'][1]:.4f}]\")\n",
    "print(f\"   • Latitude CI: [{spatial_stats['ci_center_y'][0]:.4f}, {spatial_stats['ci_center_y'][1]:.4f}]\")\n",
    "print(f\"   • Standard distance: {spatial_stats['standard_distance']:.4f}\")\n",
    "print(f\"   • Std distance CI: [{spatial_stats['ci_standard_distance'][0]:.4f}, {spatial_stats['ci_standard_distance'][1]:.4f}]\")\n",
    "\n",
    "if 'morans_i' in locals():\n",
    "    print(f\"\\n3. SPATIAL AUTOCORRELATION:\")\n",
    "    print(f\"   • Moran's I: {bid_autocorr['morans_i']:.4f}\")\n",
    "    print(f\"   • Z-score: {bid_autocorr['z_score']:.4f}\")\n",
    "    print(f\"   • P-value: {bid_autocorr['p_value']:.4f}\")\n",
    "    print(f\"   • Interpretation: {'Significant clustering' if bid_autocorr['p_value'] < 0.05 else 'Random spatial pattern'}\")\n",
    "\n",
    "if 'gi_stats' in locals():\n",
    "    print(f\"\\n4. HOT SPOT ANALYSIS:\")\n",
    "    print(f\"   • Significant hot/cold spots: {np.sum(gi_p_values < 0.05)} ({100*np.sum(gi_p_values < 0.05)/len(gi_p_values):.1f}%)\")\n",
    "    print(f\"   • Maximum Gi* statistic: {gi_stats.max():.4f}\")\n",
    "    print(f\"   • Minimum Gi* statistic: {gi_stats.min():.4f}\")\n",
    "    print(f\"   • Hot spots detected: {np.sum((gi_stats > 1.65) & (gi_p_values < 0.05))}\")\n",
    "    print(f\"   • Cold spots detected: {np.sum((gi_stats < -1.65) & (gi_p_values < 0.05))}\")\n",
    "\n",
    "if 'cluster_labels' in locals():\n",
    "    print(f\"\\n5. SPATIAL CLUSTERING:\")\n",
    "    print(f\"   • Optimal clusters identified: {n_clusters}\")\n",
    "    print(f\"   • Noise points: {n_noise} ({100*n_noise/len(spatial_df):.1f}%)\")\n",
    "    print(f\"   • Clustering efficiency: {100*(len(spatial_df) - n_noise)/len(spatial_df):.1f}%\")\n",
    "    if best_params:\n",
    "        print(f\"   • Optimal eps parameter: {best_params['eps']:.4f}\")\n",
    "        print(f\"   • Optimal min_samples: {best_params['min_samples']}\")\n",
    "\n",
    "print(f\"\\n6. STATISTICAL CONFIDENCE:\")\n",
    "print(f\"   • All confidence intervals calculated at 95% level\")\n",
    "print(f\"   • Spatial autocorrelation tested with null hypothesis of randomness\")\n",
    "print(f\"   • Hot spot analysis uses Getis-Ord Gi* with significance testing\")\n",
    "print(f\"   • Clustering parameters optimized using silhouette analysis\")\n",
    "\n",
    "print(f\"\\n7. LIMITATIONS AND ASSUMPTIONS:\")\n",
    "print(f\"   • Assumes spatial stationarity (consistent patterns across study area)\")\n",
    "print(f\"   • Point-based analysis may not reflect true lease boundaries\")\n",
    "print(f\"   • Distance calculations use Euclidean distance (appropriate for small areas)\")\n",
    "print(f\"   • Clustering assumes spatial proximity indicates similarity\")\n",
    "print(f\"   • Statistical tests assume normality of residuals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}