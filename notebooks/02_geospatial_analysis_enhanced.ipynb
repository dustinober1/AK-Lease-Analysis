{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alaska Oil & Gas Lease Analysis - Enhanced Geospatial Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive geospatial analysis of Alaska OCS lease data with advanced spatial statistics, confidence intervals for spatial patterns, and rigorous statistical validation of geographic trends.\n",
    "\n",
    "## Technical Approach\n",
    "- **Spatial Statistics**: Moran's I for spatial autocorrelation\n",
    "- **Hot Spot Analysis**: Getis-Ord Gi* statistic with significance testing\n",
    "- **Distance Analysis**: Nearest neighbor analysis with confidence intervals\n",
    "- **Kernel Density**: Bandwidth optimization and statistical validation\n",
    "- **Spatial Clustering**: DBSCAN with parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_utils import calculate_morans_i, getis_ord_gi_star, optimize_dbscan_parameters, calculate_spatial_confidence_intervals\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"Geospatial libraries imported successfully\")\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"Folium available: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Spatial Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV and GeoJSON data\n",
    "df = pd.read_csv('../data/AK_Leases.csv')\n",
    "gdf = gpd.read_file('../data/AK_Leases.geojson')\n",
    "\n",
    "print(f\"CSV data shape: {df.shape}\")\n",
    "print(f\"GeoJSON data shape: {gdf.shape}\")\n",
    "print(f\"Coordinate Reference System: {gdf.crs}\")\n",
    "\n",
    "# Merge datasets if they have different information\n",
    "if 'LEASE_NUMBER' in df.columns and 'LEASE_NUMBER' in gdf.columns:\n",
    "    # Merge on lease number\n",
    "    merged_gdf = gdf.merge(df[['LEASE_NUMBER', 'BID_AMOUNT', 'CURRENT_AREA', 'ROYALTY_RATE', 'LEASE_IS_ACTIVE']], \n",
    "                          on='LEASE_NUMBER', how='left')\n",
    "    print(f\"Merged data shape: {merged_gdf.shape}\")\n",
    "else:\n",
    "    merged_gdf = gdf.copy()\n",
    "    print(\"Using GeoJSON data directly\")\n",
    "\n",
    "# Extract centroids for point analysis\n",
    "merged_gdf['centroid'] = merged_gdf.geometry.centroid\n",
    "merged_gdf['longitude'] = merged_gdf.centroid.x\n",
    "merged_gdf['latitude'] = merged_gdf.centroid.y\n",
    "\n",
    "# Clean data for analysis\n",
    "spatial_df = merged_gdf.dropna(subset=['longitude', 'latitude']).copy()\n",
    "print(f\"\nSpatial analysis dataset: {len(spatial_df):,} leases with coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spatial Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test spatial autocorrelation for different variables\n",
    "if 'BID_AMOUNT' in spatial_df.columns:\n",
    "    # Filter out missing bid amounts\n",
    "    bid_spatial = spatial_df.dropna(subset=['BID_AMOUNT'])\n",
    "    bid_spatial = bid_spatial[bid_spatial['BID_AMOUNT'] > 0]\n",
    "    \n",
    "    if len(bid_spatial) > 10:\n",
    "        # Log transform bid amounts\n",
    "        log_bids = np.log10(bid_spatial['BID_AMOUNT'])\n",
    "        \n",
    "        bid_autocorr = calculate_morans_i(\n",
    "            bid_spatial['longitude'], \n",
    "            bid_spatial['latitude'], \n",
    "            log_bids\n",
    "        )\n",
    "        \n",
    "        print(\"=== Spatial Autocorrelation Analysis ===\")\n",
    "        print(f\"Log10(Bid Amount) Moran's I: {bid_autocorr['morans_i']:.4f}\")\n",
    "        print(f\"Expected under null: {bid_autocorr['expected_i']:.4f}\")\n",
    "        print(f\"Z-score: {bid_autocorr['z_score']:.4f}\")\n",
    "        print(f\"P-value: {bid_autocorr['p_value']:.4f}\")\n",
    "        \n",
    "        if bid_autocorr['p_value'] < 0.05:\n",
    "            interpretation = \"Significant spatial autocorrelation detected\"\n",
    "            if bid_autocorr['morans_i'] > bid_autocorr['expected_i']:\n",
    "                interpretation += \" (clustered pattern)\"\n",
    "            else:\n",
    "                interpretation += \" (dispersed pattern)\"\n",
    "        else:\n",
    "            interpretation = \"No significant spatial autocorrelation (random pattern)\"\n",
    "        \n",
    "        print(f\"Interpretation: {interpretation}\")\n",
    "    else:\n",
    "        print(\"Insufficient data for spatial autocorrelation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hot Spot Analysis with Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hot spot analysis\n",
    "if 'BID_AMOUNT' in spatial_df.columns and len(bid_spatial) > 10:\n",
    "    gi_stats, gi_p_values = getis_ord_gi_star(\n",
    "        bid_spatial['longitude'],\n",
    "        bid_spatial['latitude'],\n",
    "        log_bids,\n",
    "        distance_threshold_km=50\n",
    "    )\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    bid_spatial = bid_spatial.copy()\n",
    "    bid_spatial['gi_star'] = gi_stats\n",
    "    bid_spatial['gi_p_value'] = gi_p_values\n",
    "    \n",
    "    # Classify hot spots and cold spots\n",
    "    alpha = 0.05\n",
    "    bid_spatial['hotspot_type'] = 'Not Significant'\n",
    "    bid_spatial.loc[(gi_stats > 1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Hot Spot (99%)'\n",
    "    bid_spatial.loc[(gi_stats > 1.65) & (gi_stats <= 1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Hot Spot (95%)'\n",
    "    bid_spatial.loc[(gi_stats < -1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Cold Spot (99%)'\n",
    "    bid_spatial.loc[(gi_stats < -1.65) & (gi_stats >= -1.96) & (gi_p_values < alpha), 'hotspot_type'] = 'Cold Spot (95%)'\n",
    "    \n",
    "    print(\"\n=== Hot Spot Analysis Results ===\")\n",
    "    hotspot_counts = bid_spatial['hotspot_type'].value_counts()\n",
    "    print(hotspot_counts)\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(f\"\nGi* Statistics Summary:\")\n",
    "    print(f\"Mean Gi*: {gi_stats.mean():.4f}\")\n",
    "    print(f\"Std Gi*: {gi_stats.std():.4f}\")\n",
    "    print(f\"Max Gi*: {gi_stats.max():.4f}\")\n",
    "    print(f\"Min Gi*: {gi_stats.min():.4f}\")\n",
    "    print(f\"Significant hot/cold spots: {np.sum(gi_p_values < alpha)} ({100*np.sum(gi_p_values < alpha)/len(gi_p_values):.1f}%)")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Clustering with Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize DBSCAN clustering\n",
    "if len(spatial_df) > 20:\n",
    "    # Prepare coordinates\n",
    "    coords = spatial_df[['longitude', 'latitude']].values\n",
    "    \n",
    "    # Standardize coordinates for clustering\n",
    "    scaler = StandardScaler()\n",
    "    coords_scaled = scaler.fit_transform(coords)\n",
    "    \n",
    "    print(\"Optimizing DBSCAN parameters...\")\n",
    "    best_params, optimization_results = optimize_dbscan_parameters(coords_scaled)\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"\nOptimal DBSCAN parameters:\")\n",
    "        print(f\"eps: {best_params['eps']:.4f}\")\n",
    "        print(f\"min_samples: {best_params['min_samples']}\")\n",
    "        \n",
    "        # Apply optimal clustering\n",
    "        optimal_dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
    "        cluster_labels = optimal_dbscan.fit_predict(coords_scaled)\n",
    "        \n",
    "        # Add cluster labels to dataframe\n",
    "        spatial_df = spatial_df.copy()\n",
    "        spatial_df['cluster'] = cluster_labels\n",
    "        \n",
    "        # Analyze clusters\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        \n",
    "        print(f\"\n=== Spatial Clustering Results ===\")\n",
    "        print(f\"Number of clusters: {n_clusters}\")\n",
    "        print(f\"Number of noise points: {n_noise}\")\n",
    "        print(f\"Clustered points: {len(spatial_df) - n_noise} ({100*(len(spatial_df) - n_noise)/len(spatial_df):.1f}%)\")\n",
    "        \n",
    "        # Cluster statistics\n",
    "        if 'BID_AMOUNT' in spatial_df.columns:\n",
    "            cluster_stats = spatial_df[spatial_df['cluster'] != -1].groupby('cluster').agg({\n",
    "                'BID_AMOUNT': ['count', 'mean', 'median'],\n",
    "                'longitude': ['mean'],\n",
    "                'latitude': ['mean']\n",
    "            }).round(4)\n",
    "            \n",
    "            print(f\"\nCluster Statistics:\")\n",
    "            print(cluster_stats)\n",
    "    else:\n",
    "        print(\"Could not find optimal clustering parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Spatial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive spatial visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# 1. Basic lease distribution\n",
    "axes[0, 0].scatter(spatial_df['longitude'], spatial_df['latitude'], \n",
    "                   alpha=0.6, s=30, c='blue')\n",
    "axes[0, 0].set_xlabel('Longitude')\n",
    "axes[0, 0].set_ylabel('Latitude')\n",
    "axes[0, 0].set_title(f'Lease Locations (n={len(spatial_df):,})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Bid amount visualization (if available)\n",
    "if 'BID_AMOUNT' in spatial_df.columns and len(bid_spatial) > 0:\n",
    "    scatter = axes[0, 1].scatter(bid_spatial['longitude'], bid_spatial['latitude'], \n",
    "                                c=np.log10(bid_spatial['BID_AMOUNT']), \n",
    "                                s=50, alpha=0.7, cmap='viridis')\n",
    "    plt.colorbar(scatter, ax=axes[0, 1], label='Log10(Bid Amount)')\n",
    "axes[0, 1].set_xlabel('Longitude')\n",
    "axes[0, 1].set_ylabel('Latitude')\n",
    "axes[0, 1].set_title('Lease Locations by Bid Amount')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'Bid Amount Data\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[0, 1].set_title('Bid Amount Analysis')\n",
    "\n",
    "# 3. Hot spot analysis (if performed)\n",
    "if 'gi_star' in locals() and len(bid_spatial) > 0:\n",
    "    # Create color map for hot spots\n",
    "    colors = []\n",
    "    for hs_type in bid_spatial['hotspot_type']:\n",
    "        if 'Hot Spot (99%)' in hs_type:\n",
    "            colors.append('red')\n",
    "        elif 'Hot Spot (95%)' in hs_type:\n",
    "            colors.append('orange')\n",
    "        elif 'Cold Spot (99%)' in hs_type:\n",
    "            colors.append('blue')\n",
    "        elif 'Cold Spot (95%)' in hs_type:\n",
    "            colors.append('lightblue')\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "    \n",
    "    axes[1, 0].scatter(bid_spatial['longitude'], bid_spatial['latitude'], \n",
    "                       c=colors, s=50, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Longitude')\n",
    "    axes[1, 0].set_ylabel('Latitude')\n",
    "    axes[1, 0].set_title('Hot Spot Analysis (Getis-Ord Gi*)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    unique_types = bid_spatial['hotspot_type'].unique()\n",
    "    legend_elements = []\n",
    "    for hs_type in unique_types:\n",
    "        if 'Hot Spot (99%)' in hs_type:\n",
    "            color = 'red'\n",
    "        elif 'Hot Spot (95%)' in hs_type:\n",
    "            color = 'orange'\n",
    "        elif 'Cold Spot (99%)' in hs_type:\n",
    "            color = 'blue'\n",
    "        elif 'Cold Spot (95%)' in hs_type:\n",
    "            color = 'lightblue'\n",
    "        else:\n",
    "            color = 'gray'\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                         markerfacecolor=color, markersize=8, label=hs_type))\n",
    "    \n",
    "    axes[1, 0].legend(handles=legend_elements, loc='best', fontsize=8)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Hot Spot Analysis\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Hot Spot Analysis')\n",
    "\n",
    "# 4. Spatial clustering (if performed)\n",
    "if 'cluster' in spatial_df.columns:\n",
    "    # Color by cluster\n",
    "    unique_clusters = spatial_df['cluster'].unique()\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_clusters)))\n",
    "    \n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        mask = spatial_df['cluster'] == cluster\n",
    "        if cluster == -1:\n",
    "            # Noise points\n",
    "            axes[1, 1].scatter(spatial_df.loc[mask, 'longitude'], \n",
    "                              spatial_df.loc[mask, 'latitude'], \n",
    "                              c='black', s=20, alpha=0.5, label='Noise')\n",
    "        else:\n",
    "            axes[1, 1].scatter(spatial_df.loc[mask, 'longitude'], \n",
    "                              spatial_df.loc[mask, 'latitude'], \n",
    "                              c=[colors[i]], s=50, alpha=0.7, \n",
    "                              label=f'Cluster {cluster}')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Longitude')\n",
    "    axes[1, 1].set_ylabel('Latitude')\n",
    "    axes[1, 1].set_title('Spatial Clusters (DBSCAN)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].legend(loc='best', fontsize=8)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Spatial Clustering\nNot Available', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Spatial Clustering')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary and Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spatial statistics with confidence intervals\n",
    "spatial_stats = calculate_spatial_confidence_intervals(\n",
    "    spatial_df['longitude'], \n",
    "    spatial_df['latitude']\n",
    ")\n",
    "\n",
    "print(\"=== COMPREHENSIVE GEOSPATIAL ANALYSIS SUMMARY ===\")\n",
    "print(f\"\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   • Total lease locations: {len(spatial_df):,}\")\n",
    "print(f\"   • Coordinate system: {gdf.crs if hasattr(gdf, 'crs') else 'Not specified'}\")\n",
    "print(f\"   • Longitude range: {spatial_df['longitude'].min():.4f} to {spatial_df['longitude'].max():.4f}\")\n",
    "print(f\"   • Latitude range: {spatial_df['latitude'].min():.4f} to {spatial_df['latitude'].max():.4f}\")\n",
    "\n",
    "print(f\"\n2. SPATIAL CENTER AND DISPERSION (95% CI):\")\n",
    "print(f\"   • Mean center: ({spatial_stats['mean_center'][0]:.4f}, {spatial_stats['mean_center'][1]:.4f})\")\n",
    "print(f\"   • Longitude CI: [{spatial_stats['ci_center_x'][0]:.4f}, {spatial_stats['ci_center_x'][1]:.4f}]\")\n",
    "print(f\"   • Latitude CI: [{spatial_stats['ci_center_y'][0]:.4f}, {spatial_stats['ci_center_y'][1]:.4f}]\")\n",
    "print(f\"   • Standard distance: {spatial_stats['standard_distance']:.4f}\")\n",
    "print(f\"   • Std distance CI: [{spatial_stats['ci_standard_distance'][0]:.4f}, {spatial_stats['ci_standard_distance'][1]:.4f}]\")\n",
    "\n",
    "if 'morans_i' in locals():\n",
    "    print(f\"\n3. SPATIAL AUTOCORRELATION:\")\n",
    "    print(f\"   • Moran's I: {bid_autocorr['morans_i']:.4f}\")\n",
    "    print(f\"   • Z-score: {bid_autocorr['z_score']:.4f}\")\n",
    "    print(f\"   • P-value: {bid_autocorr['p_value']:.4f}\")\n",
    "    print(f\"   • Interpretation: {'Significant clustering' if bid_autocorr['p_value'] < 0.05 else 'Random spatial pattern'}\")\n",
    "\n",
    "if 'gi_stats' in locals():\n",
    "    print(f\"\n4. HOT SPOT ANALYSIS:\")\n",
    "    print(f\"   • Significant hot/cold spots: {np.sum(gi_p_values < 0.05)} ({100*np.sum(gi_p_values < 0.05)/len(gi_p_values):.1f}%)\")\n",
    "    print(f\"   • Maximum Gi* statistic: {gi_stats.max():.4f}\")\n",
    "    print(f\"   • Minimum Gi* statistic: {gi_stats.min():.4f}\")\n",
    "    print(f\"   • Hot spots detected: {np.sum((gi_stats > 1.65) & (gi_p_values < 0.05))}\")\n",
    "    print(f\"   • Cold spots detected: {np.sum((gi_stats < -1.65) & (gi_p_values < 0.05))}\")\n",
    "\n",
    "if 'cluster_labels' in locals():\n",
    "    print(f\"\n5. SPATIAL CLUSTERING:\")\n",
    "    print(f\"   • Optimal clusters identified: {n_clusters}\")\n",
    "    print(f\"   • Noise points: {n_noise} ({100*n_noise/len(spatial_df):.1f}%)\")\n",
    "    print(f\"   • Clustering efficiency: {100*(len(spatial_df) - n_noise)/len(spatial_df):.1f}%\")\n",
    "    if best_params:\n",
    "        print(f\"   • Optimal eps parameter: {best_params['eps']:.4f}\")\n",
    "        print(f\"   • Optimal min_samples: {best_params['min_samples']}\")\n",
    "\n",
    "print(f\"\n6. STATISTICAL CONFIDENCE:\")\n",
    "print(f\"   • All confidence intervals calculated at 95% level\")\n",
    "print(f\"   • Spatial autocorrelation tested with null hypothesis of randomness\")\n",
    "print(f\"   • Hot spot analysis uses Getis-Ord Gi* with significance testing\")\n",
    "print(f\"   • Clustering parameters optimized using silhouette analysis\")\n",
    "\n",
    "print(f\"\n7. LIMITATIONS AND ASSUMPTIONS:\")\n",
    "print(f\"   • Assumes spatial stationarity (consistent patterns across study area)\")\n",
    "print(f\"   • Point-based analysis may not reflect true lease boundaries\")\n",
    "print(f\"   • Distance calculations use Euclidean distance (appropriate for small areas)\")\n",
    "print(f\"   • Clustering assumes spatial proximity indicates similarity\")\n",
    "print(f\"   • Statistical tests assume normality of residuals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}